# GuÃ­a de ContribuciÃ³n - ETL Retail Claims

## ğŸ¯ Objetivos del Proyecto

Este proyecto implementa un pipeline ETL completo para procesamiento de reclamos retail usando Google Cloud Platform.

## ğŸ“ Estructura del Proyecto

```
etl-retail-claims/
â”œâ”€â”€ dags/                          # DAGs de Airflow
â”œâ”€â”€ cloud_functions/               # Cloud Functions (ingesta)
â”œâ”€â”€ dataproc/                      # Jobs de Spark
â”‚   â”œâ”€â”€ jobs/                      # Scripts PySpark
â”‚   â””â”€â”€ configs/                   # Configuraciones
â”œâ”€â”€ bigquery/                      # Esquemas y procedimientos
â”‚   â”œâ”€â”€ schemas/                   # DDL de tablas
â”‚   â””â”€â”€ stored_procedures/         # Stored procedures
â”œâ”€â”€ config/                        # Configuraciones del proyecto
â”œâ”€â”€ tests/                         # Tests unitarios e integraciÃ³n
â”œâ”€â”€ scripts/                       # Scripts de utilidad
â””â”€â”€ monitoring/                    # ConfiguraciÃ³n de alertas
```

## ğŸ› ï¸ ConfiguraciÃ³n del Ambiente

### 1. Prerequisites
```bash
# Instalar Python 3.8+
python --version

# Instalar Google Cloud SDK
curl https://sdk.cloud.google.com | bash
exec -l $SHELL
gcloud init

# Configurar credenciales
gcloud auth application-default login
```

### 2. Setup Local
```bash
# Clonar repositorio
git clone <repo-url>
cd etl-retail-claims

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Copiar configuraciÃ³n
cp config/secrets_template.yaml config/secrets.yaml
cp .env.example .env
```

## ğŸš€ Desarrollo

### Crear una Nueva Feature

```bash
# 1. Crear rama
git checkout -b feature/tu-feature

# 2. Realizar cambios
# 3. Ejecutar tests
pytest tests/ -v

# 4. Commitear cambios
git add .
git commit -m "feat: descripciÃ³n de cambios"

# 5. Push y crear Pull Request
git push origin feature/tu-feature
```

### Patrones de CÃ³digo

#### Cloud Functions
- Usar clases para lÃ³gica reutilizable
- Incluir logging en todos los mÃ©todos
- Manejo robusto de excepciones
- Retornar JSON con status y mensajes claros

#### PySpark Jobs
- Usar funciones de pyspark.sql para optimizaciÃ³n
- Loguear paso a paso la transformaciÃ³n
- Incluir validaciÃ³n de datos
- Regresar reportes de calidad

#### SQL Procedures
- Usar MERGE para inserciones/actualizaciones
- Crear tablas temporales para lÃ³gica compleja
- Incluir comentarios en reglas de negocio
- Optimizar con Ã­ndices

#### DAGs Airflow
- Usar operadores especÃ­ficos del provider (no BashOperator)
- Incluir PythonOperator para validaciones
- Manejar XCom para pasar datos entre tareas
- Usar trigger_rule apropiadamente

## ğŸ§ª Testing

### Ejecutar Tests
```bash
# Tests unitarios
pytest tests/unit/ -v

# Con coverage
pytest tests/unit/ --cov=. --cov-report=html

# Tests de integraciÃ³n (requiere GCP)
pytest tests/integration/ -v
```

### Escribir Tests
```python
import unittest

class TestMyFeature(unittest.TestCase):
    def test_something(self):
        # Arrange
        input_data = {"key": "value"}
        
        # Act
        result = my_function(input_data)
        
        # Assert
        self.assertEqual(result, expected_value)
```

## ğŸ“Š Despliegue

### A Staging
```bash
# Revisar cambios
git diff main

# Ejecutar tests en CI
# (Los tests corren automÃ¡ticamente en GitHub Actions)

# Desplegar a staging (si pruebas pasan)
bash scripts/deploy_gcp.sh your-staging-project-id
```

### A ProducciÃ³n
```bash
# Crear release
git tag -a v1.0.0 -m "Release 1.0.0"
git push origin v1.0.0

# Desplegar a producciÃ³n
bash scripts/deploy_gcp.sh your-production-project-id
```

## ğŸ“‹ Checklist de Desarrollo

- [ ] CÃ³digo sigue PEP8
- [ ] Tests incluidos y pasando
- [ ] Docstrings en funciones
- [ ] Logging en puntos clave
- [ ] Manejo de errores
- [ ] Actualizado README si es necesario
- [ ] Commits con mensajes claros

## ğŸ› Reportar Bugs

1. Verificar que el bug no estÃ© reportado
2. Crear issue con:
   - TÃ­tulo descriptivo
   - Pasos para reproducir
   - Comportamiento esperado
   - Comportamiento actual
   - Logs/screenshots

## ğŸ’¡ Proponer Features

1. Crear discussion primero
2. Describir:
   - Objetivo de la feature
   - Casos de uso
   - Impacto en el pipeline
   - Requerimientos tÃ©cnicos

## ğŸ“š Recursos

- [Google Cloud Documentation](https://cloud.google.com/docs)
- [Apache Airflow](https://airflow.apache.org/)
- [PySpark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [BigQuery Documentation](https://cloud.google.com/bigquery/docs)

## âœ‰ï¸ Contacto

Data Engineering Team - data-alerts@company.com

---

Â¡Gracias por contribuir al proyecto! ğŸ‰
