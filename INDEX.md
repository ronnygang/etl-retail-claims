# üìë √çndice de Archivos - Proyecto ETL Retail Claims

## üìÅ Estructura Completa del Proyecto

### üè† Ra√≠z del Proyecto
```
etl-retail-claims/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ instructions/
‚îÇ       ‚îî‚îÄ‚îÄ instructions.instructions.md     # Gu√≠a de desarrollo en espa√±ol
‚îú‚îÄ‚îÄ .env.example                             # Variables de entorno de ejemplo
‚îú‚îÄ‚îÄ .gitignore                               # Archivos a ignorar en git
‚îú‚îÄ‚îÄ README.md                                # Documentaci√≥n principal
‚îú‚îÄ‚îÄ QUICKSTART.md                            # Gu√≠a de inicio r√°pido
‚îú‚îÄ‚îÄ CONTRIBUTING.md                          # Gu√≠a de contribuci√≥n
‚îú‚îÄ‚îÄ CICD.md                                  # Documentaci√≥n de Cloud Build CI/CD
‚îú‚îÄ‚îÄ GITHUB_INTEGRATION.md                    # Integraci√≥n con GitHub
‚îú‚îÄ‚îÄ DEPLOYMENT.md                            # Runbook de despliegue manual
‚îú‚îÄ‚îÄ MONITORING.md                            # Monitoreo y alertas
‚îú‚îÄ‚îÄ requirements.txt                         # Dependencias Python
‚îî‚îÄ‚îÄ sample_claims.jsonl                      # Datos de ejemplo
```

### ‚òÅÔ∏è Google Cloud Platform

#### Cloud Functions (Ingesta SFTP)
```
cloud_functions/
‚îî‚îÄ‚îÄ ingest_sftp_to_gcs/
    ‚îú‚îÄ‚îÄ main.py                              # C√≥digo principal
    ‚îî‚îÄ‚îÄ requirements.txt                     # Dependencias espec√≠ficas
```
**Prop√≥sito**: Descargar JSON desde SFTP y cargar a GCS
**Entrada**: Archivos JSON desde servidor SFTP
**Salida**: Archivos JSON en `gs://bucket/bronze/retail-claims/`

#### Google Cloud Storage (GCS)
- **Estructura**: `gs://retail-claims-etl/bronze/retail-claims/{YYYY}/{MM}/{DD}/`
- **Datos**: Archivos JSON crudos (capa Bronze)
- **Formato**: NEWLINE_DELIMITED_JSON

#### BigQuery

##### Capa Bronze (Datos Raw)
```
bigquery/schemas/bronze_external_table.sql
```
- **Dataset**: `retail_claims_bronze`
- **Tabla**: `claims_external` (tabla externa referenciando GCS)
- **Prop√≥sito**: Fuente de datos para transformaciones

##### Capa Silver (Datos Estructurados)
```
bigquery/schemas/silver_schema.sql
```
- **Dataset**: `retail_claims_silver`
- **Tabla**: `claims_structured`
- **Transformaciones**: Limpieza, estandarizaci√≥n, validaci√≥n de calidad
- **Particionamiento**: Por `processing_date`
- **Clustering**: Por `customer_id`, `store_id`

##### Capa Gold (Reglas de Negocio)
```
bigquery/schemas/gold_schema.sql
```
- **Dataset**: `retail_claims_gold`
- **Tabla**: `claims_business_rules`
- **Transformaciones**: Clasificaci√≥n, escalaci√≥n, score de riesgo
- **Particionamiento**: Por `processing_date`
- **Clustering**: Por `customer_id`, `claim_priority`, `requires_escalation`

##### Stored Procedures
```
bigquery/stored_procedures/silver_to_gold_business_rules.sql
```
- **Nombre**: `sp_silver_to_gold_transformation()`
- **Reglas**:
  - Clasificaci√≥n por monto (LOW, MEDIUM, HIGH, CRITICAL)
  - Escalaci√≥n autom√°tica (PENDING > 7 d√≠as o monto > $2000)
  - Score de riesgo (0.1 - 1.5)
  - Categorizaci√≥n por per√≠odo

### üîÑ Dataproc (PySpark)

#### Jobs PySpark
```
dataproc/jobs/
‚îî‚îÄ‚îÄ bronze_to_silver_transform.py            # Transformaci√≥n Bronze‚ÜíSilver
```
**Prop√≥sito**: Procesar datos de Bronze, aplicar transformaciones, escribir en Silver
**Entrada**: Tabla externa `claims_external` (Bronze)
**Salida**: Tabla `claims_structured` (Silver)
**Transformaciones**:
1. Lectura desde BigQuery
2. Limpieza y estandarizaci√≥n
3. Agregaci√≥n de columnas t√©cnicas
4. Validaci√≥n de calidad
5. Escritura en BigQuery

#### Configuraci√≥n
```
dataproc/configs/
‚îî‚îÄ‚îÄ dataproc_cluster_config.yaml             # Config del cluster
```
**Especificaciones**:
- Master: `n1-standard-4` (1 instancia)
- Workers: `n1-standard-4` (2 instancias)
- Spark: 4 cores, 8GB RAM por executor

### üéØ Orquestaci√≥n (Cloud Composer / Airflow)

#### DAGs
```
dags/
‚îî‚îÄ‚îÄ retail_claims_etl_dag.py                 # DAG principal
```
**Nombre**: `retail_claims_etl_pipeline`
**Cronograma**: Diariamente a las 2:00 AM UTC
**Tareas**:
1. `log_pipeline_start` - Registrar inicio
2. `ingest_sftp_to_gcs` - Ejecutar Cloud Function
3. `validate_ingestion` - Validar ingesta exitosa
4. `create_dataproc_cluster` - Crear cluster Spark
5. `bronze_to_silver_transformation` - Ejecutar job PySpark
6. `delete_dataproc_cluster` - Eliminar cluster
7. `silver_to_gold_business_rules` - Ejecutar Stored Procedure
8. `log_pipeline_end` - Registrar finalizaci√≥n

### üß™ Testing

#### Tests Unitarios
```
tests/unit/
‚îú‚îÄ‚îÄ __init__.py
‚îî‚îÄ‚îÄ test_transformations.py                  # Tests de l√≥gica de negocio
```
**Cobertura**:
- Clasificaci√≥n de prioridad (LOW, MEDIUM, HIGH, CRITICAL)
- L√≥gica de escalaci√≥n
- C√°lculo de score de riesgo

#### Tests de Integraci√≥n
```
tests/integration/
‚îî‚îÄ‚îÄ __init__.py
```
(Espacio para pruebas end-to-end con GCP)

### ‚öôÔ∏è Configuraci√≥n

#### Variables de Entorno
```
.env.example                                 # Plantilla de variables
```
**Variables principales**:
- `GCP_PROJECT_ID` - ID del proyecto GCP
- `GCS_BUCKET_NAME` - Nombre del bucket
- `SFTP_HOST`, `SFTP_USERNAME`, `SFTP_PASSWORD` - Credenciales SFTP
- `BQ_DATASET_*` - Nombres de datasets BigQuery

#### Configuraci√≥n de Desarrollo
```
config/
‚îú‚îÄ‚îÄ environment.yaml                         # Config general del proyecto
‚îî‚îÄ‚îÄ secrets_template.yaml                    # Plantilla de secretos
```

### üìö Documentaci√≥n

```
README.md                                    # Documentaci√≥n completa
QUICKSTART.md                                # Gu√≠a de inicio r√°pido (5 min)
CONTRIBUTING.md                              # Gu√≠a de contribuci√≥n y desarrollo
```

### üõ†Ô∏è Scripts

#### Despliegue Automatizado
```
scripts/
‚îî‚îÄ‚îÄ deploy_gcp.sh                            # Script de despliegue a GCP
```
**Funciones**:
- Crear buckets GCS
- Crear datasets BigQuery
- Ejecutar scripts DDL
- Desplegar Cloud Function
- Subir job Spark
- Crear entorno Composer
- Subir DAG

### üìä Monitoreo

```
monitoring/                                  # (Directorio para alertas)
```

---

## üìã Listado de Archivos Creados

### Python (`.py`)
- `cloud_functions/ingest_sftp_to_gcs/main.py` - Cloud Function
- `dataproc/jobs/bronze_to_silver_transform.py` - Job PySpark
- `dags/retail_claims_etl_dag.py` - DAG Airflow
- `tests/unit/test_transformations.py` - Tests unitarios
- `tests/__init__.py`, `tests/unit/__init__.py`, `tests/integration/__init__.py` - Paquetes Python

### SQL (`.sql`)
- `bigquery/schemas/bronze_external_table.sql` - Tabla externa Bronze
- `bigquery/schemas/silver_schema.sql` - Tabla Silver
- `bigquery/schemas/gold_schema.sql` - Tabla Gold
- `bigquery/stored_procedures/silver_to_gold_business_rules.sql` - Stored Procedure

### YAML (`.yaml`)
- `config/environment.yaml` - Configuraci√≥n del proyecto
- `config/secrets_template.yaml` - Plantilla de secretos
- `dataproc/configs/dataproc_cluster_config.yaml` - Config de Dataproc

### Markdown (`.md`)
- `README.md` - Documentaci√≥n principal
- `QUICKSTART.md` - Inicio r√°pido
- `CONTRIBUTING.md` - Gu√≠a de contribuci√≥n
- `INDEX.md` - Este archivo

### Configuraci√≥n
- `.env.example` - Variables de entorno
- `.gitignore` - Archivos a ignorar
- `requirements.txt` - Dependencias Python

### Datos
- `sample_claims.jsonl` - Datos de ejemplo en JSONL

### Shell (`.sh`)
- `scripts/deploy_gcp.sh` - Script de despliegue

---

## üîó Dependencias Entre Componentes

```
SFTP
  ‚Üì
Cloud Function (main.py)
  ‚Üì
GCS (Bronze)
  ‚Üì
BigQuery External Table (bronze_external_table.sql)
  ‚Üì
PySpark Job (bronze_to_silver_transform.py)
  ‚Üì
BigQuery Silver Table (silver_schema.sql)
  ‚Üì
Stored Procedure (silver_to_gold_business_rules.sql)
  ‚Üì
BigQuery Gold Table (gold_schema.sql)
```

---

## üìà Matriz de Datos

| Componente | Entrada | Salida | Formato | Partici√≥n |
|-----------|---------|--------|--------|-----------|
| Cloud Function | SFTP JSON | GCS | JSON | `YYYY/MM/DD/` |
| Bronze (External) | GCS | BigQuery Query | JSON | - |
| Dataproc Job | BigQuery Bronze | BigQuery Silver | Parquet | `processing_date` |
| Gold Procedure | BigQuery Silver | BigQuery Gold | Parquet | `processing_date` |

---

## ‚úÖ Checklist de Verificaci√≥n

- [x] Directorios creados
- [x] Cloud Function implementada
- [x] Esquemas BigQuery definidos
- [x] Job PySpark implementado
- [x] DAG Airflow configurado
- [x] Tests unitarios incluidos
- [x] Documentaci√≥n completa
- [x] Scripts de despliegue
- [x] Ejemplos de datos
- [x] Configuraci√≥n de secretos

---

## üöÄ Pr√≥ximos Pasos

1. **Configurar credenciales GCP**
   ```bash
   gcloud auth application-default login
   ```

2. **Llenar archivo `.env`**
   ```bash
   cp .env.example .env
   vim .env
   ```

3. **Ejecutar tests locales**
   ```bash
   pytest tests/unit/ -v
   ```

4. **Desplegar en GCP**
   ```bash
   bash scripts/deploy_gcp.sh your-project-id
   ```

---

**Documento generado**: 2024-01-01
**√öltima actualizaci√≥n**: 2025-11-12
